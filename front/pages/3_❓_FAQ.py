import streamlit as st
import asyncio
import os
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from dotenv import load_dotenv
load_dotenv()
API_KEY = os.getenv("API_KEY")
# í˜ì´ì§€ ì œëª©
st.markdown("<h1 style='text-align: center;'>EVë¥¼ í•„ìš”ë¡œ í•˜ëŠ” ë‹¹ì‹ , \n ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”!</h1>", unsafe_allow_html=True)
st.write("---")

# ì§ˆë¬¸ê³¼ ë‹µë³€ ë¦¬ìŠ¤íŠ¸
faq_list = [
    ("Q1. ì°¨ëŠ” ì–¼ë§ˆì¸ê°€ìš”?", "ğŸš— ì „ê¸°ì°¨ ê°€ê²©ì€ ëª¨ë¸ê³¼ ì‚¬ì–‘ì— ë”°ë¼ ë‹¤ë¥´ì§€ë§Œ, ì¼ë°˜ì ìœ¼ë¡œ 4,000ë§Œ ì›ì—ì„œ 1ì–µ ì› ì´ìƒì…ë‹ˆë‹¤."),
    ("Q2. ì¶©ì „ ì‹œê°„ì€ ì–¼ë§ˆë‚˜ ê±¸ë¦¬ë‚˜ìš”?", "âš¡ ê¸‰ì† ì¶©ì „ì€ ì•½ 30ë¶„~1ì‹œê°„, ì™„ì† ì¶©ì „ì€ 6~8ì‹œê°„ ì •ë„ ì†Œìš”ë©ë‹ˆë‹¤."),
    ("Q3. ì£¼í–‰ê±°ë¦¬ëŠ” ì–¼ë§ˆë‚˜ ë˜ë‚˜ìš”?", "ğŸ›£ï¸ ìµœì‹  EV ëª¨ë¸ì€ í•œ ë²ˆ ì¶©ì „ìœ¼ë¡œ ìµœëŒ€ 500km ì´ìƒ ì£¼í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."),
    ("Q4. ë°°í„°ë¦¬ ìˆ˜ëª…ì€ ì–¼ë§ˆë‚˜ ë˜ë‚˜ìš”?", "ğŸ”‹ EV ë°°í„°ë¦¬ëŠ” í‰ê·  8~10ë…„ ì‚¬ìš©ì´ ê°€ëŠ¥í•˜ë©°, ì œì¡°ì‚¬ì— ë”°ë¼ ë³´ì¦ ê¸°ê°„ì´ ì œê³µë©ë‹ˆë‹¤."),
    ("Q5. ì¶©ì „ì†ŒëŠ” ì–´ë””ì—ì„œ ì°¾ì„ ìˆ˜ ìˆë‚˜ìš”?", "ğŸ—ºï¸ ì „êµ­ ê³ ì†ë„ë¡œ íœ´ê²Œì†Œ, ëŒ€í˜• ë§ˆíŠ¸, ê³µê³µê¸°ê´€ ë“± ë‹¤ì–‘í•œ ì¥ì†Œì—ì„œ ì¶©ì „ì†Œë¥¼ ì´ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."),
    ("Q6. EV ìœ ì§€ë¹„ëŠ” ì–¼ë§ˆë‚˜ ë“œë‚˜ìš”?", "ğŸ’¸ ì „ê¸°ì°¨ëŠ” ì—°ë£Œë¹„ì™€ ìœ ì§€ë¹„ê°€ ë‚´ì—°ê¸°ê´€ ì°¨ëŸ‰ì— ë¹„í•´ ìµœëŒ€ 70% ì ˆê°ë©ë‹ˆë‹¤."),
    ("Q7. êµ­ê³  ë³´ì¡°ê¸ˆì€ ì–¼ë§ˆë‚˜ ë°›ì„ ìˆ˜ ìˆë‚˜ìš”?", "ğŸ’° êµ­ê³  ë³´ì¡°ê¸ˆì€ ëª¨ë¸ê³¼ ì§€ì—­ì— ë”°ë¼ ë‹¤ë¥´ë©°, ìµœëŒ€ 800ë§Œ ì›ê¹Œì§€ ì§€ì›ë©ë‹ˆë‹¤."),
    ("Q8. ê²¨ìš¸ì²  ì£¼í–‰ê±°ë¦¬ëŠ” ì–¼ë§ˆë‚˜ ê°ì†Œí•˜ë‚˜ìš”?", "â„ï¸ ê²¨ìš¸ì² ì—ëŠ” ë‚œë°©ìœ¼ë¡œ ì¸í•´ ì£¼í–‰ê±°ë¦¬ê°€ ì•½ 10~20% ê°ì†Œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."),
    ("Q9. EVì˜ ì¶©ì „ ë¹„ìš©ì€ ì–¼ë§ˆì¸ê°€ìš”?", "ğŸ”Œ ì¶©ì „ ë¹„ìš©ì€ kWhë‹¹ ì•½ 300~400ì›ìœ¼ë¡œ, ë‚´ì—°ê¸°ê´€ ëŒ€ë¹„ ë§¤ìš° ê²½ì œì ì…ë‹ˆë‹¤."),
    ("Q10. EVëŠ” ì–¼ë§ˆë‚˜ ì•ˆì „í•œê°€ìš”?", "ğŸ›¡ï¸ EVëŠ” ë‚®ì€ ë¬´ê²Œ ì¤‘ì‹¬ê³¼ ìµœì‹  ì•ˆì „ ê¸°ìˆ  ë•ë¶„ì— ë§¤ìš° ì•ˆì „í•©ë‹ˆë‹¤. ëŒ€ë¶€ë¶„ì˜ ëª¨ë¸ì€ ìµœê³  ì•ˆì „ ë“±ê¸‰ì„ íšë“í–ˆìŠµë‹ˆë‹¤.")
]



# FAQ í† ê¸€ ìƒì„±
for question, answer in faq_list:
    with st.expander(question):
        st.write(answer)

# í•˜ë‹¨ ë¼ì¸
st.write("---")
st.markdown("<h5 style='text-align: center;'>ğŸ“ ì¶”ê°€ ë¬¸ì˜ëŠ” ê³ ê°ì„¼í„°ë¡œ ì—°ë½ì£¼ì„¸ìš”.\n [ê²½ê¸°ë„ ê´‘ì£¼ì§€ì ]010-1234-5678</h5>", unsafe_allow_html=True)


if "chat_history" not in st.session_state:
    st.session_state.chat_history = []


user_input = st.chat_input("ì…ë ¥")

# localDB.insertData('Me',user_input)

def format_chat_history():
    history_text = ""
    for message in st.session_state.chat_history:
        if message["role"] == "user":
            history_text += f"ë‚˜: {message['content']}\n"
        else:
            history_text += f"AI: {message['content']}\n"
    return history_text

ai_prompt = ChatPromptTemplate.from_messages([
    ("system", "ë„ˆëŠ” ê¸°ë³¸ì ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ëŒ€ë‹µí•´ì•¼í•´ ê·¸ë¦¬ê³  ì „ê¸°ìë™ì°¨ ë˜ëŠ” ìë™ì°¨ì— ëŒ€í•œ FAQë¥¼ ì œì™¸í•˜ê³  ë‹¤ë¥¸ ë‚´ìš©ì„ ë¬¼ì–´ë³¼ ê²½ìš°ì—ëŠ” ë‹µí•  ìˆ˜ ì—†ë‹¤ê³  í•´"),
    ("user", "ëŒ€í™” ë‚´ìš©:\n{chat_history}\në‚˜: {prompt}\nAI:")
])

llm = ChatGoogleGenerativeAI(
    model="gemini-1.5-pro",
    google_api_key=API_KEY,
    temperature=0
)

output_parser = StrOutputParser()

chain = ai_prompt | llm.with_config({"run_name": "model"}) | output_parser.with_config({"run_name": "Assistant"})

def display_chat_history():
    """ëŒ€í™” ë‚´ì—­ ì „ì²´ë¥¼ ì¶œë ¥ (ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì „ê¹Œì§€ ê³ ì •)"""
    for message in st.session_state.chat_history:
        role = "ë‚˜" if message["role"] == "user" else "AI"
        st.write(f"**{role}:** {message['content']}")

async def generate_response(user_text, ai_placeholder):
    """ì´ì „ ëŒ€í™” ë‚´ìš©ê³¼ í•¨ê»˜ AI ì‘ë‹µì„ ìŠ¤íŠ¸ë¦¬ë°"""
    result_text = ""
    conversation_history = format_chat_history()
    prompt_vars = {"chat_history": conversation_history, "prompt": user_text}
    
    async for chunk in chain.astream_events(prompt_vars, version="v1", include_names=["Assistant"]):
        if chunk.get("event") in ["on_parser_start", "on_parser_stream"]:
            if "data" in chunk:
                data = chunk["data"]
                if isinstance(data, dict):
                    data = data.get("chunk", "")
                result_text += data
                
                ai_placeholder.markdown(f"**AI:** {result_text}")
                
    st.session_state.chat_history.append({"role": "assistant", "content": result_text})
    return result_text

if user_input:
    st.session_state.chat_history.append({"role": "user", "content": user_input})
    
    display_chat_history()
    
    ai_placeholder = st.empty()
    
    try:
        asyncio.run(generate_response(user_input, ai_placeholder))
    except Exception as e:
        st.error(f"ì—ëŸ¬ ë°œìƒ: {e}")
